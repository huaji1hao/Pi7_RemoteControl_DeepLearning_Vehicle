# NUS 2024 SWS Pi_SmartGuide_Vehicle

[English](README.md)

## 项目内容

在Robotics部分，使用了Raspberry Pi和Arduino Mega 2560。Raspberry Pi主要使用其外部的摄像头插件，后期利用这一摄像头实现图像识别和模型判断。Arduino部分利用其多样的数据库集成多个功能模块如LED显示、基于L239D的电机控制、舵机控制等，实现机器人运动的各环节控制。此外，使用串口进行两个板块的通信。

此车还包括深度学习的模型识别，涉及到传输协议及通信、环境搭建、网页设计、参数调整拟合等一系列工作。

## 项目成员

### 核心成员

- **UESTC_FANGYUZHANG**（Robotics）：硬件模块功能设计，串口通信设计
- **UNNC_JUNFENGZHU**（Robotics）：电机驱动封装，网页交互设计

## 💎 技术实现

### Baseline

人工驾驶小车能在实验室的迷宫中找到藏起来的“小猫”，利用车载的Picamera进行识别，判断小猫的种类并在网页上输出识别结果，考验小车驱动的流畅性、识别的准确性和寻找的策略。

- 使用HTTP协议实现大模型与网页视频实时内容进行通信，展示小车对各类猫的识别结果与识别率。
- 通过串口通信实现在网页上对小车的控制，监听键盘通过串口返回键盘输入值控制小车的基本行驶。
- 使用Flask框架，通过视频流的形式将树莓派上的摄像头内容同步到网页上实现前后端交互。
- Arduino封装小车行驶逻辑，通过差速控制小车转向，封装串口接收模块，分层式控制小车前进后退和转向。

实现逻辑：通过WASD操控小车寻找小猫，到小猫照片面前可以实时识别出种类和显示准确率，按“R”可以记录本次识别结果到数据库中，按“X”可以即刻停止小车运动。最终在本次Baseline测验中，由于寻找策略的原因我们只找到了6/8张小猫照片但是识别成功率为100%，算是圆满完成。

### Advance

利用DL和robot的相关技术满足需求，重点关注的是服务场景，而不是实现技术。

#### 智能导盲小车

旨在为视障人士提供一种经济实惠且技术先进的替代方案，以替代传统导盲犬的工作。这一项目的核心目标是通过集成深度学习、遥控机器人、精确导航、语音交互等多项技术，实现在已知信息的地图中，用智能小车引导盲人前行。

- 黑线循迹，采用了三个循迹模块。
- 超声波避障。
- Dijkstra算法进行路径规划，我们预先输入起点和终点，小车自动规划出最优路径并带你前往目的地。
- 实现逻辑：小车（树莓派）和server端通过socket套接字进行通信（app.py），小车通过（py_control.py）进行导航，（trialmap.py）进行Dijkstra算法规划最短路径，（get_distance.py）可以获取小车与路口的距离，提前判断下一个路口的转向信息，并且获取当前摄像头中黑线的倾斜度从而判断小车转向是否到位，防止转向之后离开黑线而无法进行下一步操作，到达目的地后会识别到地上的终点线并停止，通过（voice.py）告知身后的主人目的地到了。

## 🚀 未来计划

#### Donkeycar：

利用我们已经封装好的硬件操作逻辑，优化串口通信延迟，通过对Donkeycar的理解，在预定路线上驾驶小车，记录下每一秒左右两轮的速度，并在该秒内拍摄20张照片（树莓派进行），再将记录下的数据放入大模型中进行训练（主机进行），再将训练结果返回到树莓派中，使得小车能自行在预定的航线中运行，实现固定路线智能导航。

#### 原来项目的拓展：

- 加入避障优化，陀螺仪计算角度实现PID精准控制，使得转向操作更加精准。
- 使用十路红外模块对黑线循迹进行优化，同样引入PID控制，使小车在黑线上平滑转弯和走弧线。

#### 更先进的技术—真正的导盲机器狗：

- 有交互式的连接把手，能确保跟牵引者的实时交互，以免出现突发状况。
- SLAM技术+车载GPS技术，能实时对周围的地形进行分析判断，并提前规划好路线。
